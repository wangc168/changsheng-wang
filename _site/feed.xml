<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-07-12T00:32:20-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Yihua Zhang</title><subtitle>Don&apos;t waste your life. Don&apos;t waste it.
</subtitle><entry><title type="html">[NeurIPS22] Advancing Model Pruning via Bi-level Optimization</title><link href="http://localhost:4000/blog/2022/bip-neurips22/" rel="alternate" type="text/html" title="[NeurIPS22] Advancing Model Pruning via Bi-level Optimization" /><published>2022-11-26T14:00:00-07:00</published><updated>2022-11-26T14:00:00-07:00</updated><id>http://localhost:4000/blog/2022/bip-neurips22</id><content type="html" xml:base="http://localhost:4000/blog/2022/bip-neurips22/"><![CDATA[<h4 id="dilemma-in-model-pruning-effective-or-efficient">Dilemma in Model Pruning: Effective or Efficient?</h4>

<p>Among the many powerful pruning methods, Iterative Magnitude Pruning (IMP) is one of the most significant and popular methods, which prunes the model in an iterative manner and can reach an extremely sparse level without any performance loss. However, it often consumes much more than training a dense model from scratch. In contrast, efficient one-shot pruning methods can not deliver a high-quality sparse subnetwork, as shown in Figure 1.
Therefore, existing pruning methods have reached a dilemma over choosing between the effective method and the efficient method.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/bip_nips22/dilemma-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/bip_nips22/dilemma-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/bip_nips22/dilemma-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/bip_nips22/dilemma.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Dilemma in Pruning" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 1. The dilemma in model pruning: Powerful pruning methods (e.g., IMP) suffer from high computational overhead.
</div>

<hr />

<center>
<b>
How to advance the optimization foundation of model pruning to achieve high accuracy and pruning efficiency?
</b>
<br />
</center>

<hr />

<h4 id="model-pruning-as-a-bi-level-optimization-problem">Model Pruning as a Bi-level Optimization Problem</h4>

<p>We start our research by revisiting the model pruning problem and reformulating it to a bi-level optimization (BLO) problem:</p>

\[\min_{\mathbf{m} \in \mathcal{S}} \ell(\mathbf{m} \odot \boldsymbol\theta^{\ast}(\mathbf{m})) \quad \quad \text{subject to} \,\,\, \boldsymbol\theta^{\ast}(\mathbf{m}) = \text{argmin}_{\boldsymbol\theta \in \mathbb{R}^n}\, \ell(\mathbf{m} \odot \boldsymbol\theta + \frac{\gamma}{2}\|\boldsymbol\theta\|_2^2),\]

<p>where the upper-level problem optimizes the pruning mask for the model and the lower-level retrains the model with the fixed mask. The benefits from this bi-level formulation are two-folded.</p>

<ul>
  <li>
    <p>First, we have the flexibility to use the mismatched pruning and retraining objectives.</p>
  </li>
  <li>
    <p>Second, the bi-level optimization enables us to explicitly optimize the coupling between the retrained model weights and the pruning mask through the implicit gradient (IG)-based optimization routine.</p>
  </li>
</ul>

<h4 id="optimization-foundation-of-bip">Optimization Foundation of BIP</h4>

<p>BLO is different from other optimization problems, as the gradient descent of the upper-level variable will involve the calculation of the implicit gradient (IG)：</p>

\[\frac{d\ell(\mathbf{m}\odot\boldsymbol\theta^\ast(\mathbf{m}))}{d\mathbf{m}} = \nabla_{\mathbf{m}}\ell(\mathbf{m}\odot\boldsymbol\theta^\ast(\mathbf{m})) + \frac{d{\boldsymbol\theta^\ast(\mathbf{m})}^\top}{d\mathbf{m}} \nabla_{\boldsymbol\theta}\ell(\mathbf{m}\odot\boldsymbol\theta^\ast(\mathbf{m})).\]

<p>The IG challenge is a fingerprint of the BLO solver and derives from the implicit function theory. It refers to the gradient of the lower-level solution w.r.t. the upper-level variable and in most cases is very difficult to calculate. The main reason is it usually involves the second-order derivative and matrix inversion:</p>

\[\frac{d{\boldsymbol\theta^\ast(\mathbf{m})}^\top}{d\mathbf{m}} = -\nabla^2_{\mathbf{m}\boldsymbol\theta}\ell(\mathbf{m}\odot\boldsymbol\theta^\ast)[\nabla^2_{\boldsymbol\theta}\ell(\mathbf{m}\odot\boldsymbol\theta^\ast) + \gamma \mathbf{I}]^{-1}.\]

<p>Very luckily, in the model pruning scenario, the upper- and lower-level variables are always combined as bi-linear variables. We can thus leverage this bi-linear property and derive a closed-form IG solution, which only requires the first-order derivative:</p>

\[\frac{d{\boldsymbol\theta^\ast(\mathbf{m})}^\top}{d\mathbf{m}} = -\frac{1}{\gamma} \text{diag}(\nabla_{\mathbf{z}}\ell(\mathbf{z})|_{\mathbf{z} = \mathbf{m} \odot \boldsymbol\theta^\ast}).\]

<p>This also makes our later proposed bi-level pruning algorithm a purely first-order method.</p>

<hr />

<h4 id="bip-bi-level-optimization-based-pruning">BiP: Bi-level Optimization-based Pruning</h4>

<p>We propose our bi-level pruning algorithm (BiP). We adopt the alternating optimization procedure by updating the upper- and lower-variable in turn.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/bip_nips22/algorithm-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/bip_nips22/algorithm-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/bip_nips22/algorithm-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/bip_nips22/algorithm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="BiP algorithm overview" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 2. The pseudo-code for Bi-level Pruning (BiP) Algorithm.
</div>

<h5 id="more-details-of-bip">More details of BiP</h5>

<p>We show the illustration of the BiP algorithm in Figure 3 below. We start with a pretrained model and some mask initialization. For the lower level, we use stochastic gradient descent to update the model parameter with the fixed mask.
For the upper level, as the mask is supposed to contain either 0 or 1, representing whether a parameter should be removed or retained, we first relax the “binary” masking “variables” to “continuous” masking “scores”, which can be updated with auto-differentiation in most of the deep learning algorithms. In the forward path, we project the scores onto the discrete constraint using hard thresholding, where the top k elements are set to 1s and the others to 0s. Thus, we summarize our upper-level problem as stochastic projected gradient descent, in short SPGD.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/bip_nips22/bip_overview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/bip_nips22/bip_overview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/bip_nips22/bip_overview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/bip_nips22/bip_overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="BiP algorithm visualization" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 3. An illustration of the proposed BiP algorithm.
</div>

<h5 id="comparison-to-imp-and-omp">Comparison to IMP and OMP</h5>

<p>We also show the illustration of the iterative magnitude pruning (IMP) and one-shot magnitude pruning (OMP) below for comparison.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/bip_nips22/imp-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/bip_nips22/imp-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/bip_nips22/imp-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/bip_nips22/imp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="BiP algorithm overview" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 4. Pruning pipeline visualization of IMP.
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/bip_nips22/omp-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/bip_nips22/omp-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/bip_nips22/omp-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/bip_nips22/omp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="BiP algorithm overview" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 5. Pruning pipeline visualization of OMP.
</div>

<hr />

<h4 id="experiment-results">Experiment results</h4>

<h5 id="bip-identifies-high-accuracy-subnetworks-in-unstructured-pruning">BIP identifies high-accuracy subnetworks in unstructured pruning</h5>

<p>In Figure 6 below, we show the unstructured pruning trajectory (given by test accuracy vs. pruning ratio) of BIP and baseline methods in 8 model-dataset setups. BiP outperforms its baselines in nearly all the settings and also finds the sparsest winning tickets nearly every time. For some settings like CIFAR-10 with model VGG-16, BiP is even capable of finding winning tickets with a pruning ratio over 90%</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/bip_nips22/exp_unstructured_pruning-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/bip_nips22/exp_unstructured_pruning-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/bip_nips22/exp_unstructured_pruning-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/bip_nips22/exp_unstructured_pruning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Unstructured pruning result." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 6. Unstructured pruning trajectory given by test accuracy (%) vs. sparsity (%) on various (dataset, model) pairs. The performance of the dense model and the best winning ticket is marked using dashed lines in each plot. The solid line and shaded area of each pruning method represent the mean and variance of test accuracies over 3 trials.
</div>

<h5 id="bip-identifies-high-accuracy-subnetworks-in-structured-pruning">BIP identifies high-accuracy subnetworks in structured pruning</h5>

<p>A similar phenomenon can be observed in the structured pruning setting as well.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/bip_nips22/exp_structured_pruning-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/bip_nips22/exp_structured_pruning-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/bip_nips22/exp_structured_pruning-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/bip_nips22/exp_structured_pruning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Structured pruning result." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 7. Filter-wise pruning trajectory given by test accuracy (%) vs. sparsity (%). Other settings strictly follow Figure 3.
</div>

<h5 id="bip-achieves-high-pruning-efficiency">BiP achieves high pruning efficiency.</h5>

<p>Next, we show the high efficiency of BiP compared to the state-of-the-art IMP method <a href="#refer-anchor-1">[1]</a>. As we have longed for, BiP consumes sparsity-agnostic time consumptions just like the one-shot pruning methods and can be 3~7 times faster than IMP.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/bip_nips22/exp_time-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/bip_nips22/exp_time-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/bip_nips22/exp_time-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/bip_nips22/exp_time.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Structured pruning result." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 8. Time consumption comparison on (CIFAR-10, ResNet-18) with different pruning ratios p.
</div>

<h5 id="bip-requires-no-rewinding">BiP requires no rewinding.</h5>

<p>Another advantage of BIP is that it insensitive to model rewinding to
find matching subnetworks. Recall that rewinding is a strategy used in Lottery Ticket Hypothesis <a href="#refer-anchor-2">[2]</a> to determine what model initialization should be used for retraining a pruned model. In Figure 9, we show the test accuracy of the BIP-pruned model when it is retrained at different rewinding epochs under various datasets and model architectures. As we can see, a carefully-tuned rewinding scheme does not lead to a significant improvement over BIP without retraining. This suggests that the subnetworks found by BIP are already of high quality and do not require any rewinding operation.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/bip_nips22/no_rewinding-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/bip_nips22/no_rewinding-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/bip_nips22/no_rewinding-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/bip_nips22/no_rewinding.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="BiP requires no rewinding." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 9. The sensitivity of BIP to rewinding epoch numbers on different datasets and model architectures. "N/A" in the x-axis indicates BIP without retraining.
</div>

<hr />

<h4 id="citation">Citation</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>@inproceedings{zhang2022advancing,
  title = {Advancing Model Pruning via Bi-level Optimization},
  author = {Zhang, Yihua and Yao, Yuguang and Ram, Parikshit and Zhao, Pu and Chen, Tianlong and Hong, Mingyi and Wang, Yanzhi and Liu, Sijia},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2022}
}
</pre></td></tr></tbody></table></code></pre></div></div>
<hr />

<h4 id="reference">Reference</h4>

<div id="refer-anchor-1"></div>
<p>[1] Xiaolong Ma et al. “Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?” NeurIPS 2021.</p>

<div id="refer-anchor-2"></div>
<p>[2] Jonathan Frankle et al. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” ICLR 2019.</p>]]></content><author><name>&lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://www.yihua-zhang.com/&apos;&gt;Yihua Zhang&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;*, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://www.cse.msu.edu/~yaoyugua/&apos;&gt;Yuguang Yao&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;*, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://rithram.github.io/&apos;&gt;Parikshit Ram&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://puzhao.info/&apos;&gt;Zhao Pu&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://tianlong-chen.github.io/about/&apos;&gt;Tianlong Chen&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://people.ece.umn.edu/~mhong/mingyi.html&apos;&gt;Mingyi Hong&lt;/a&gt;&lt;sup&gt;[5]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://web.northeastern.edu/yanzhiwang/&apos;&gt;Yanzhi Wang&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://lsjxjtu.github.io/&apos;&gt;Sijia Liu&lt;/a&gt;&lt;sup&gt;[1,2]&lt;/sup&gt;</name></author><category term="neurips22" /><category term="ScalableML" /><summary type="html"><![CDATA[Dilemma in Model Pruning: Effective or Efficient?]]></summary></entry><entry><title type="html">[NeurIPS22] Fairness Reprogramming</title><link href="http://localhost:4000/blog/2022/fairness-reprogramming-neurips22/" rel="alternate" type="text/html" title="[NeurIPS22] Fairness Reprogramming" /><published>2022-11-26T14:00:00-07:00</published><updated>2022-11-26T14:00:00-07:00</updated><id>http://localhost:4000/blog/2022/fairness-reprogramming-neurips22</id><content type="html" xml:base="http://localhost:4000/blog/2022/fairness-reprogramming-neurips22/"><![CDATA[<h3 id="overview">Overview</h3>

<p>In this paper, we propose a new generic fairness learning paradigm,
called fairness reprogramming:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/overview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/overview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/overview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Dilemma in Pruning" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 1. An example of fairness reprogramming in CV and NLP tasks. The input agnostic trigger can promote fairness without altering the pretrained model.
</div>

<hr />

<h3 id="principal-research-question">Principal Research Question</h3>

<center>
<b>
Can an unfair model be reprogrammed into a fair one? <br /> If so, why and how would it work?
</b>
<br />
</center>

<hr />

<h3 id="fairness-reprogramming">Fairness Reprogramming</h3>

<p>Consider a classification task, where \(\mathbf{X}\) represents the input feature and \(Y\) represents the output label. There exists some sensitive attributes or demographic groups, \(Z\), that may be spuriously
correlated with \(Y\). There is a pre-trained classifier, \(f^*(\cdot)\) that predicts \(Y\) from \(\mathbf{X}\), <em>i.e.</em>, \(\hat{Y} = f^*(\mathbf{X})\).</p>

<p>The goal of fairness reprogramming is to improve the fairness of the classifier by modifying the input \(\mathbf{X}\), while keeping the classifier’s weights \(\boldsymbol\theta\) fixed. In particular, we aim to achieve either of the following fairness criteria.</p>

<ul>
  <li>Equalized Odds:</li>
</ul>

\[\hat{Y} \perp Z | Y,\]

<ul>
  <li>Demographic Parity:</li>
</ul>

\[\hat{Y} \perp Z,\]

<p>where \(\perp\) denotes independence.</p>

<h4 id="fairness-trigger">Fairness Trigger</h4>

<p>The reprogramming primarily involves appending a fairness trigger to the input. Formally, the input modification takes the following generic form:</p>

\[\tilde{\mathbf{X}} = m(\mathbf{X}; \boldsymbol\theta, \boldsymbol \delta) = [\boldsymbol \delta, g(\mathbf{X}; \boldsymbol\theta)],\]

<p>where \(\tilde{\mathbf{X}}\) denotes the modified input; \([\cdot]\) denotes vector concatenation (see Figure 1).</p>

<h4 id="optimization-objective-and-discriminator">Optimization Objective and Discriminator</h4>

<p>Our optimization objective is as follows</p>

\[\min_{\boldsymbol\theta, \boldsymbol\delta} \,\,\, \mathcal{L}_{\text{util}} (\mathcal{D}_{\text{tune}}, f^* \circ m) + \lambda \mathcal{L}_{\text{fair}} (\mathcal{D}_{\text{tune}}, f^* \circ m),\]

<p>where \(\mathcal{D}_{\text{tune}}\) represents the dataset that is used to train the fairness trigger. The first loss term, \(\mathcal{L}_{\text{util}}\), is the utility loss function of the task. For classification tasks, \(\mathcal{L}_{\text{util}}\) is usually the cross-entropy loss, <em>i.e.</em>,:</p>

\[\mathcal{L}_{\text{util}}(\mathcal{D}_{\text{tune}}, f^* \circ m) = \mathbb{E}_{\mathbf{X}, Y \sim \mathcal{D}_{\text{tune}}} [\textrm{CE}(Y, f^*(m(\mathbf{X})))],\]

<p>The second loss term, \(\mathcal{L}_{fair}\), encourages the prediction to follow the fairness criteria and should measure how much information about \(Z\) is in \(\hat{Y}\). Thus, we introduce another network, the discriminator, \(d(\cdot; \boldsymbol \phi)\), where \(\boldsymbol \phi\) represents its parameters. If the equalized odds criterion is applied,  then \(d(\cdot; \boldsymbol \phi)\) should predict \(Z\) from \(\hat{Y}\) and \(Y\); if the demographic parity criterion is applied, then the input to \(d(\cdot; \boldsymbol \phi)\) would just be \(\hat{Y}\). The information of \(Z\) can be measured by maximizing the <em>negative</em> cross-entropy loss for the prediction of \(Z\) over the discriminator parameters:</p>

\[\mathcal{L}_{\text{fair}} (\mathcal{D}_{\text{tune}}, f^* \circ m) = \max_{\boldsymbol \phi} \mathbb{E}_{\mathbf{X}, Y, Z \sim \mathcal{D}_{\text{tune}}} [-\textrm{CE}(Z, d(f^*(m(\mathbf{X})), Y; \boldsymbol \phi))].\]

<p>We give an illustration of our fairness reprogramming algorithm below, which co-optimizes the fairness trigger and the discriminator at the same time in a min-max fashion.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-1">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/algorithm-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/algorithm-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/algorithm-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/algorithm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Algorithm." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 2. An illustration of our proposed fairness reprogramming algorithm.
</div>

<h3 id="experiment-results">Experiment results</h3>

<p>We consider the following two commonly used NLP and CV datasets:</p>

<ul>
  <li>
    <p>Civil Comments: The dataset contains 448k texts with labels that depict the toxicity of
each input. The demographic information of each text is provided.</p>
  </li>
  <li>
    <p>CelebA: The dataset contains over 200k human face images and each contains 39 binary
attribute annotations. We adopt the hair color prediction task in our experiment and use gender annotation as the demographic information.</p>
  </li>
</ul>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/main_results-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/main_results-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/main_results-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/main_results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Main results." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 3. Results on (a) Civil Comments and (b) CelebA. We report the negative DP (left) and the negative EO (right) scores. For each method, we vary the trade-off parameter λ to record the performance. The closer a dot is to the upper-right corner, the better the model is. 
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/tuning_ratio-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/tuning_ratio-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/tuning_ratio-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/tuning_ratio.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Tuning ratio." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 4. Results on (a) Civil Comments and (b) CelebA with different tuning data ratios. We report the negative DP (left) and negative EO (right) scores. We consider a fixed BASE model trained with a training set, whose negative bias scores are presented as a black dashed line. Then we train other methods with different tuning data ratios to promote fairness of the BASE model.
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/transfer-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/transfer-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/transfer-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/transfer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Transfer setting." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 5. Results in the transfer setting. We report negative DP (left) and negative EO (right) scores. The triggers are first trained in a BASE model. Then, we evaluate the triggers based on another unseen BASE model. We change the parameter λ to trade off accuracy with fairness and draw the curves in the same way as Figure 3.
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/multi_class-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/multi_class-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/multi_class-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/multi_class.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Multi-class setting." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 6. Performance of multi-class classification. For (a) and (b), we use the attributes Blond Hair, Smiling, and Attractive for multi-class construction. We add an additional attribute Wavy Hair for (c) and (d).
</div>

<hr />

<h3 id="why-does-fairness-trigger-work">Why does Fairness Trigger work?</h3>

<p>In our paper, we both theoretically prove and empirically demonstrate why a <em>global trigger</em> can obscure the demographic information for <em>any</em> input. In general, the trigger learned by the reprogrammer contains very strong demographic information and blocks the model from relying on the real demographic information from the input. Since the same trigger is attached to all the input, the uniform demographic information contained in the trigger will weaken the dependence of the model on the true demographic information contained in the data, and thus improve the fairness of the pretrained model.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/why_works-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/why_works-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/why_works-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/why_works.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Trigger demographic information." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 7. Illustration of why fairness trigger works. (a) The data generation process. (b) The information flow from data to the classifier through sufficient statistics. (c) A fairness trigger strongly indicative of a demographic group can confuse the classifier with a false demographic posterior, thus preventing the classifier from using the correct demographic information.
</div>

<h4 id="input-saliency-analysis">Input Saliency Analysis</h4>

<p>The following two figures compare the saliency maps of some example inputs with and without the fairness triggers. Specifically, For the NLP applications, we extract a subset of Civil
Comments with religion-related demographic annotations, and apply IG to localize word pieces
that contribute most to the text toxicity classification. For the CV application, we use GradCam to identify class-discriminative regions of CelebA’s test images.</p>

<p>Figure 8 presents the input saliency maps on two input images with respect to their predicted labels, non-blond hair and blond hair, respectively. When there is no fairness trigger, the saliency region incorrectly concentrates on the facial parts, indicating the classifier is likely to use biased information, such as gender, for its decision. With the fairness trigger, the saliency region moves to the hair parts.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/gradcam-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/gradcam-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/gradcam-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/gradcam.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Grad Cam." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 8. Gradient-based saliency map visualized with GradCam [\[1\]](#refer-anchor-1) of different methods. The highlighted zones (marked in red) depicting regions exerting major influence on the predicted labels (non-blond hair v.s. blond hair) in each row, which also depict the attention of the model on the input image.
</div>

<p>In Figure 9, our fairness trigger consists of a lot of religion-related words (e.g., diocesan, hebrew, parish). Meanwhile, the predicted toxicity score of the benign text starting from ‘muslims’ significantly reduces. These observations verify our theoretical hypothesis that the fairness trigger is strongly indicative of a certain demographic group to prevent the classifier from using the true demographic information.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/integrated_grad-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/integrated_grad-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/integrated_grad-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/integrated_grad.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Integrated Gradient." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 9. A text example from Civil Comments with Integrated Gradient [\[2,3\]](#refer-anchor-2) highlighting important words that influence ERM model predictions. The text is concatenated with three triggers generated with different adversary weights. Green highlights the words that lean toward toxic predictions and red highlights non-toxic leaning words. The model prediction tends to be correct after adding the triggers.
</div>

<p>To further verify that the triggers encode demographic information, we trained a demographic classifier to predict the demographics from the input (texts or images). We use the demographic classifier to predict the demographic information of a null image/text with the trigger. We see that the demographic classifier gives confident outputs on the triggers, indicating that they found triggers are highly indicative of demographics.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/fairness_nips22/demographic_info_trigger-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/fairness_nips22/demographic_info_trigger-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/fairness_nips22/demographic_info_trigger-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/fairness_nips22/demographic_info_trigger.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Trigger demographic information." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 10. Predictions of the demographic classifier on a null input with triggers generated by different λ. The demographic prediction for CV triggers indicate the predicted score for Male and Female, and it is Christian, Muslim, and other religion for NLP.
</div>

<hr />

<h3 id="citation">Citation</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>@inproceedings{zhang2022fairness,
  title = {Fairness reprogramming},
  author = {Zhang, Guanhua and Zhang, Yihua and Zhang, Yang and Fan, Wenqi and Li, Qing and Liu, Sijia and Chang, Shiyu},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2022}
}
</pre></td></tr></tbody></table></code></pre></div></div>
<hr />

<h3 id="reference">Reference</h3>

<div id="refer-anchor-1"></div>
<p>[1] Ramprasaath R Selvaraju et al. “Grad-cam: Visual explanations from deep networks via gradient-based localization” ICCV 2017.</p>

<div id="refer-anchor-2"></div>
<p>[2] Mukund Sundararajan et al. “Axiomatic attribution for deep networks” ArXiv, vol. abs/1703.01365, 2017.</p>

<div id="refer-anchor-3"></div>
<p>[3] Narine Kokhlikyan et al. “Captum: A unified and generic model interpretability library for PyTorch” arXiv preprint arXiv:2009.07896.</p>]]></content><author><name>&lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://ghzhang233.github.io/&apos;&gt;Guanhua Zhang&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;*, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://www.yihua-zhang.com/&apos;&gt;Yihua Zhang&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;*, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://https://scholar.google.com/citations?hl=zh-CN&amp;user=_-5PSgQAAAAJ/&apos;&gt;Yang Zhang&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://wenqifan03.github.io/&apos;&gt;Wenqi Fan&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://scholar.google.com/citations?hl=zh-CN&amp;user=XRB2rKIAAAAJ&apos;&gt;Qing Li&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://lsjxjtu.github.io/&apos;&gt;Sijia Liu&lt;/a&gt;&lt;sup&gt;[2,4]&lt;/sup&gt;</name></author><category term="neurips22" /><category term="TrustworthyML" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">[CVPR22] Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free</title><link href="http://localhost:4000/blog/2022/trojan-lth-cvpr22/" rel="alternate" type="text/html" title="[CVPR22] Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free" /><published>2022-06-21T14:00:00-07:00</published><updated>2022-06-21T14:00:00-07:00</updated><id>http://localhost:4000/blog/2022/trojan-lth-cvpr22</id><content type="html" xml:base="http://localhost:4000/blog/2022/trojan-lth-cvpr22/"><![CDATA[<h4 id="motivation">Motivation</h4>

<p>As models usually learn “too well” during training - so much that make various types of attacks possible, backdoor attack (or Trojan Attack) has become a real-life threat to the AI model deployed in the real world. At the same time, extensive research work on model pruning has shown that the weights of an overparameterized model (e.g., DNN) can be pruned without hampering its generalization ability. Combining both lines of research, our story begins with the following question:</p>

<center>
<b>

How does the model sparsity relate to its train-time robustness against Trojan attacks?
</b>
<br />
</center>

<hr />

<h4 id="the-discovery-of-winning-trojan-ticket">The Discovery of ‘Winning Trojan Ticket’</h4>

<p>We start our research by making an observation on the performance change of a backdoored model as we gradually increase the model sparsity with the model pruning technique. More specifically, we would like to see how the clean accuracy (CA) as well as attack success rate (ASR) will change with respect to the increasing sparsity. We plot the curve of CA and ASR w.r.t. sparsity ratio in Figure 1. Some interesting phenomena we listed below indicate that there is a strong connection between model sparsity and Trojan features.</p>

<p>As ASR is constantly higher than CA, and CA drops much faster than ASR, Trojan features learned by backdoored attacks are significantly more stable against pruning than benign features. Therefore, we assume that “Trojan attacks can be uncovered through the pruning dynamics of the Trojan model”. The question remains: how to leverage the ‘stubbornness’ of the Trojan features to detect the Trojan attack itself?</p>

<p>As we further increase the model sparsity, both ASR and CA reasonably fall to a relatively low level. However, at a certain sparsity level, the ASR surges to a conspicuously high level while the CA remains low, which we term the ‘winning Trojan Ticket’, borrowing the idea from LTH-oriented <a href="#refer-anchor-1">[1]</a> iterative magnitude pruning. The performance of the winning Trojan Ticket implies that such a model subnetwork preserves the Trojan attack traces while retaining chance-level performance on clean inputs. In other words, it is a ‘purely bad’ subnetwork.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/backdoor_cvpr22/overview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/backdoor_cvpr22/overview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/backdoor_cvpr22/overview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/backdoor_cvpr22/overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 1. An overview of our proposal: Trojan features learned by backdoored attacks are significantly more stable against pruning than benign features. Therefore, Trojan attacks can be uncovered through the pruning dynamics of the Trojan model. Weight pruning identifies the ‘winning Trojan ticket’, which can be used for Trojan detection and recovery. 
</div>

<p>Leveraging LTH-oriented iterative magnitude pruning (IMP), the ‘winning Trojan Ticket’ can be discovered, which preserves the Trojan attack performance while retaining chance-level performance on clean inputs.</p>

<p>Thus, the existence of the ‘winning Trojan Ticket’ could serve as an indicator of Trojan attacks. However, in real-world applications, it is hard for the users to acquire the ASR (namely the red curve in Figure 1), as the attack information is transparent to the users. Thus, we need to find a substitute indicator for ASR, which does not require any attack information or even clean data.</p>

<p>The winning Trojan ticket can be detected by our proposed linear model connectivity (LMC)-based Trojan score.</p>

<hr />

<h4 id="trojan-score-linear-mode-connectivity-based-trojan-indicator">Trojan Score: Linear Mode Connectivity-based Trojan Indicator</h4>

<p>We adopt Linear Mode Connectivity <a href="#refer-anchor-2">[2]</a> (LMC) to measure the stability of the Trojan ticket \(\phi := (m \odot \theta)\) v.s. the \(k\)-step finetuned Trojan ticket \(\phi := (m \odot \theta^{(k)})\).</p>

<p>We define the Trojan Score as</p>

\[\mathcal{S}_{Trojan} = \max_{\alpha \in [0, 1]} \mathcal{E} (\alpha \phi - (1 - \alpha)\phi_{k}) - \frac{\mathcal{\phi} - \mathcal{\phi_k}}{2}\]

<p>where the first term denotes LMC and the second term an error baseline. \(\mathcal{E}(\phi)\) denotes the training error of the model \(\phi\).</p>

<p>A sparse network with the peak Trojan Score maintains the highest ASR in the extreme pruning regime and is termed as the Winning Trojan Ticket.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/backdoor_cvpr22/pruning_dynamic-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/backdoor_cvpr22/pruning_dynamic-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/backdoor_cvpr22/pruning_dynamic-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/backdoor_cvpr22/pruning_dynamic.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 2. The pruning dynamics of Trojan ticket (dash line) and 10-step finetuned ticket (solid line) on CIFAR-10 with ResNet-20 and gray-scale backdoor trigger. For comparison, the Trojan score is also reported.
</div>

<hr />

<h4 id="trojan-trigger-reverse-engineer">Trojan Trigger Reverse Engineer</h4>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/backdoor_cvpr22/trigger_l1_norm-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/backdoor_cvpr22/trigger_l1_norm-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/backdoor_cvpr22/trigger_l1_norm-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/backdoor_cvpr22/trigger_l1_norm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 3. The \(\ell_1\) norm values of recovered Trojan triggers for all labels. The plot title signifies network architecture, trigger type, and the images for reverse engineering on CIFAR-10. Class “1” is the true target label for Trojan attacks. Green check or red cross indicates whether the detected label (with the least \(\ell_1\) norm matches the true target label).
</div>

<hr />

<h4 id="trigger-reverse-engineer">Trigger Reverse Engineer</h4>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/backdoor_cvpr22/recover_trigger_poster-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/backdoor_cvpr22/recover_trigger_poster-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/backdoor_cvpr22/recover_trigger_poster-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/backdoor_cvpr22/recover_trigger_poster.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 4. Visualization of recovered Trojan trigger patterns from dense Trojan models (baseline) and winning Trojan tickets. ResNet-20s on CIFAR-10 with RGB triggers are used. The first column shows the random seed images used for trigger recovery.
</div>

<hr />

<h4 id="citation">Citation</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>@inproceedings{chen2022quarantine,
  title = {Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free},
  author = {Chen, Tianlong and Zhang, Zhenyu and Zhang, Yihua and Chang, Shiyu and Liu, Sijia and Wang, Zhangyang},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {598--609},
  year = {2022}
}
</pre></td></tr></tbody></table></code></pre></div></div>
<hr />

<h4 id="reference">Reference</h4>

<div id="refer-anchor-1"></div>
<p>[1] Jonathan Frankle et al. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” ICLR 2019.</p>

<div id="refer-anchor-2"></div>
<p>[2] Jonathan Frankle et al. “Linear Mode Connectivity and the Lottery Ticket Hypothesis.” ICML 2020.</p>

<div id="refer-anchor-3"></div>
<p>[3] Ren Wang et al. “Practical detection of trojan neural networks: Data-limited and data-free cases.” ECCV 2020.</p>]]></content><author><name>&lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://tianlong-chen.github.io/&apos;&gt;Tianlong Chen&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;*, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://scholar.google.com/citations?user=ZLyJRxoAAAAJ&amp;hl=zh-CN&apos;&gt;Zhenyu Zhang&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;*, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://www.yihua-zhang.com/&apos;&gt;Yihua Zhang&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt;*, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://code-terminator.github.io/&apos;&gt;Shiyu Chang&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://lsjxjtu.github.io/&apos;&gt;Sijia Liu&lt;/a&gt;&lt;sup&gt;[2,4]&lt;/sup&gt;, &lt;a style=&apos;color: #dfebf7&apos; href=&apos;https://vita-group.github.io/&apos;&gt;Zhangyang Wang&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt;</name></author><category term="cvpr22" /><category term="TrustworthyML" /><summary type="html"><![CDATA[Motivation]]></summary></entry></feed>